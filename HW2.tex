\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,verbatim,bbm}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ib}{\end{enumerate}}
\newcommand{\p}{\item}
	
\newcommand{\bm}[1]{\boldsymbol{#1}}
\begin{document}

\title{{\bf Homework Assignment 2}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
Please upload the HW on canvas by 10pm Nov 8th. Please type up your homework using latex. We will not accept handwritten homeworks\footnote{Two of these homeworks were adapted from A. Dimakis and C. Caramanis's class}. 
%\textbf{Set algebra and probability laws.}

\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}x
%\newpage
\p Consider a design matrix $\bm{X}$ such that $\bm{X}$ has orthonormal columns, i.e.
$\bm{X^T X} = \bm{I}$, where $\bm{I}$ is the $p \times p$ identity matrix. Consider the following regularization:
\begin{equation}
\underset{\beta}{\min}\frac{1}{2}(\bm{X\beta-y})^T(\bm{X\beta-y})+\lambda||\bm{\beta}||_m
\end{equation} %where $||\bm{\bm{\beta}}||_0 = \sum_{i=1}^p \mathbbm{1}(\beta_i\neq 0)$.

\bi
\p Derive the solution to equation $(1)$ for $m=2$ (ridge regression).
\p Derive the solution to equation $(1)$ for $m=1$ (lasso).
\p When $m=0$, the penalty involves $||\bm{\bm{\beta}}||_0 = \sum_{i=1}^p \mathbbm{1}(\beta_i\neq 0)$. 
\bi
\p Is this a convex optimization problem? Why or why not?
\p Show that the solution to equation $(1)$ with $m=0$ is given by $\bm{\Tilde{\beta}}$, where
\[
\Tilde{\beta}_i = 
\begin{cases}
\bm{v_i^T y},& \text{if } |\bm{v_i^T y}|>\sqrt{2\lambda}\\
0            & \text{if } |\bm{v_i^T y}|\leq\sqrt{2\lambda}
\end{cases}
\]
This is also called the hard thresholding estimator. $\bm{v_i}$ is the $i^{th}$ column of $\bm{X}$.
\ib
\ib

\p 
 Consider the following problem of fused Lasso, or total variation de-noising. 
$$\min_x \frac{1}{2}\|x-z\|_2^2+\lambda \sum_{i=1}^{n-1}|x_{i+1}-x_i|.$$
Here, $z$ is a noisy signal and $\lambda$ is non-negative.
\bi
\p Write this problem as follows:
$$\min_x \frac{1}{2}\|x-z\|_2^2+\lambda \|Dx\|_1.$$
where $D$ is a $n-1\times n$ matrix. What is $D$?
\p Write down the subgradient of the objective function. 
\p Implement the subgradient descent algorithm. On the noisy.txt dataset, apply the algorithm and show the convergence plot against number of iterations. Play with different stepsizes and discuss how that affects the convergence.
\p The problem can be written as
\begin{align}
\min_x &\frac{1}{2}\|x-z\|_2^2+\lambda \|y\|_1\notag\\
\mbox{s.t. }& y=Dx
\end{align}
Show that the dual of the above problem is:
\begin{align}
&\min_u \frac{1}{2}u^T DD^Tu-u^TDz.\notag\\
& \mbox{s.t. } \|u\|_\infty\leq \lambda 
\end{align}
\p Now implement the proximal gradient algorithm for the dual problem. In order to do this, you may need to look at the proximal operator given below:
\begin{align*}
\text{prox}_t(x)&=\arg\min_z \frac{\|x-z\|^2}{2t}+I_\lambda(z)
\end{align*}
where $I_\lambda(z)=0$ if $|z|\leq \lambda$ and $\infty$ otherwise. In this case, the proximal operator gets reduced to a projection operator. On the same datset, apply this algorithm and show the convergence plot against the number of iterations. How does this compare with the subgradient method you implemented before?
\p Finally, for the dual proximal gradient method, show the effect of different $\lambda$ values by plotting the denoised curve superimposed on the original noisy curve.
\ib
\iffalse
\p 	 Consider the following two similarity functions for two sets $A$ and $B$. The overlap similarity function:
$$sim_{over}(A,B)=\frac{|A\cap B|}{\min(|A|,|B|)}$$
and the Dice similarity function
$$sim_{dice}(A,B)=\frac{2|A\cap B|}{|A|+|B|}$$
Is there a locality sensitive scheme for these? Prove or give a counter example.
\p Exercises 3.6.3 and 3.6.4 from Chapter 3 of the book “Mining of Massive Datasets.”
\fi
\p Download the dataset articles-1000.txt (1.6MB), which contains 1000 articles. Each row corresponds to an article, represented by an ID (e.g., t120) and its content (e.g., The Supreme Court in Johnnesberg on Friday...).
\bi
\p Convert each article into a set of 2-shingles ( bigrams). The goal is to map each article ID to a set\footnote{You can use Python sets to do that. A set should contain unique elements.} of IDs of  shingles that article contains. 
You can do this by going over the data once as follows: for each article, first split\footnote{Use Python String split() method to do that-- no need to remove the punctuations.} it into a list of words, remove the stopwords\footnote{you can remove the stop words specified by stopwords.words("english") from nltk.corpus.}
, generate a list of 2-shingles, and then hash each shingle
to a 32-bit integer (i.e., the shingle ID) using CRC32 hash. The resulting shingle IDs range from 0 to $2^{32} - 1$. What is the total number of unique shingles across all the books? What is the average number of shingles present per book?

Here is an example of a 2-shingle and its hashed value (i.e., the shingle ID):

{\texttt{import binascii}
		
\texttt{shingle = "machine learning"}

\texttt{shingleID = binascii.crc32(shingle) \& 0xffffffff}
}
\p Generate MinHash signatures using 10 hash functions, where each is as follows:
$h(x) = c_1x + c_2 \text{ mod } p.$

Set $p = 4294967311$, which is a prime number larger than $2^{32} - 1$. Uniformly sample 10 values of $c_1, c_2 \in\{ 1, 2, . . . , p -1\}$ and compute the corresponding MinHash signatures.

For the 1st article, compare its signature vector with that of the rest of the articles and estimate their Jaccard similarity (i.e., compute the percentage of signatures that are equal). Find the book that has the largest (estimated) similarity with the 1st book then compute the actual Jaccard similarity between the two books (based on the computed shingle sets). Your result should be a triplet (bookID, estimatedJaccard, trueJaccard).

\textit{Hint: remember, you do not need to generate 10 permutations. Use the trick we learned in class, you can also find it in Section 3.3.5 of the “Mining of Massive Datasets” book linked from the class website.}

\p\textbf{Amplification} Use the LSH technique in Section 3.4 of the same book, construct $n = br$ MinHash signatures, where $b$ is the number of bands, and $r$ is the number of hash values in each band. Find all the articles that are “similar” to the 1st article (i.e., articles that agree in at least one band of signatures), and put it into a set called S (excluding the 1st article itself). Let $Sim(i, j)$ be the actual Jaccard similarity between articles $i$ and $j$. Given a
threshold $t$, define the percentage of false positives (fp) in set $S$ as
\begin{align*}
\text{False positives}=\frac{|\{i \in S : Sim(i, 1) < t\}|}{|S|}.
\end{align*}


Set t = 0.8, plot fp as a function of $b$ (for $b = 1, 3, 5, 7, 9$ with $r = 2$). Similarly, plot
fp as a function of $r$ (for $r = 1, 3, 5, 7, 9$ with $b = 10$). For each $(b, r)$ pair, plot the average value of fp over 10 realizations. Give a short comparison of the two.
\p Find the 5 most similar article pairs without any approximation. How long did it take? Now find the 5 most similar article pairs using LSH (with b,r as hyperparameters). Compare your results for different $(b,r)$ pairs.
\ib
\end{enumerate}
%\end{comment}



\end{document} 
