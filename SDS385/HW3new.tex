\documentclass[11pt]{article}
\usepackage[left=1in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,bm}
\usepackage{amsbsy,verbatim}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\DeclareMathOperator*{\argmin}{\arg\!\min}



\begin{document}

\title{{\bf Homework Assignment 3\\
Due by end of the day Dec 5th via canvas}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
%\textbf{Set algebra and probability laws.}
\begin{enumerate}
\item \textbf{Spectral Clustering\footnote{This problem was designed in collaboration with Jon Huang}}
In class we have seen k-means for estimating GMM's. Since your professor will not get a chance to actually cover her favorite clustering method aka spectral clustering, methinks this is a great opportunity to introduce you to it.
There is a class of clustering algorithms, called 
spectral clustering algorithms, which has recently 
become quite popular.  Many of these algorithms are 
quite easy to implement and perform well on 
certain clustering problems compared to more traditional 
methods like $k$-means.  In this problem, we will try to 
develop some intuition about why these approaches
make sense and implement one of these algorithms.

Before beginning, we'll review a few basic linear
algebra concepts you may find useful for some
of the problems.
\begin{itemize}
\item If $A$ is a matrix, it has an 
$v$ with eigenvalue $\lambda$ if $Av=\lambda v$.
\item For any $m\times m$ symmetric matrix $A$, 
the \emph{Singular Value Decomposition} of $A$ yields a 
factorization of $A$ into
\[
A = USU^T
\]
where U is an $m\times m$ orthogonal matrix 
(meaning that the columns are pairwise orthogonal).
and $S=diag(|\lambda_1|,|\lambda_2|,\dots,|\lambda_m|)$ 
where the $\lambda_i$ are the eigenvalues of $A$. 
\end{itemize}

Given a set of $m$ datapoints $x_1,\dots,x_m$,
the input to a spectral clustering algorithm typically
consists of a matrix, $A$, of pairwise similarities between
datapoints often called the \emph{affinity matrix}.  The choice of
how to measure similarity between points is one which is often left 
to the practitioner.  A very simple affinity matrix can be
constructed as follows:
\begin{equation}
\label{aff-eqn}
A(i,j)=A(j,i)=\left\{ \begin{array}{cc} 1 & \mbox{if $d(x_i,x_j)<\Theta$} \\
0 & \mbox{otherwise} \\
\end{array}\right.
\end{equation}
where $d(x_i,x_j)$ denotes the Euclidean distance 
between points $x_i$ and $x_j$.

The general idea of spectral clustering is to construct
a mapping of the datapoints to an eigenspace of $A$ with the 
hope that points are well separated in this eigenspace so that something
simple like $k$-means applied to these new points will perform well.
\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=2.0in]{fourdots.eps}\\
\end{center}
\caption{\label{fourdots-fig} Simple dataset}
\end{figure}

As an example, consider forming the affinity matrix for 
the dataset in Figure ~\ref{fourdots-fig} using Equation ~\ref{aff-eqn} with
$\Theta=1$.  We have that
\[
A = \left[\begin{array}{cccccc}
 & \vline & a & b & c & d \\
 \hline
a & \vline & 1 & 1 & 0 & 0 \\
b & \vline & 1 & 1 & 0 & 0\\
c & \vline & 0 & 0 & 1 & 1\\
d & \vline & 0 & 0 & 1 & 1\\
\end{array}\right]
\]

Now for this particular example, the clusters $\{a,b\}$ and $\{c,d\}$
show up as nonzero blocks in the affinity matrix.  This is, of course,
artificial, since we could have constructed the matrix $A$ using any
ordering of $\{a,b,c,d\}$.  For example, another possibile affinity matrix
for $A$ could have been:
\[
\tilde{A} = \left[\begin{array}{cccccc}
 & \vline & a & c & b & d \\
 \hline
a & \vline & 1 & 0 & 1 & 0 \\
c & \vline & 0 & 1 & 0 & 1\\
b & \vline & 1 & 0 & 1 & 0\\
d & \vline & 0 & 1 & 0 & 1\\
\end{array}\right]
\]

The key insight here is that the eigenvectors of 
matrices $A$ and $\tilde{A}$ have the same entries (just 
permuted).  The eigenvectors with nonzero eigenvalue of $A$
are: $e_1=(.7,.7,0,0)^T$, $e_2=(0,0,.7,.7)$.  And the nonzero 
eigenvectors of $\tilde{A}$ are: $\tilde{e}_1=(.7,0,.7,0)^T$, $\tilde{e}_2=(0,.7,0,.7)$.
Spectral clustering embeds the original datapoints in a new space
by using the coordinates of these eigenvectors.  Specifically, it maps the 
point $x_i$ to the point $(e_1(i),e_2(i),\dots,e_k(i))$ where $e_1,\dots,e_k$ 
are the top $k$ eigenvectors of $A$.  We refer to this mapping as the \emph{spectral embedding}.  
%See Figure ~\ref{blockfig} for an example.

%\begin{figure}[ht!]
 % \label{blockfig}
  %\begin{center}
   % \includegraphics[width=3in]{block2.eps}\\
%\end{center}
%\caption{Using eigenvectors to embed the datapoints.  Notice that the points $\{a,b,c,d\}$ are tightly 
%clustered in this space}
%	\end{figure}

%\begin{figure}
 % \begin{center}
  %  \includegraphics[width=3.2in]{purnafig.eps}\\
%\end{center}
%\caption{\label{Spectral-figs}Dataset with rectangles}
%\end{figure}

\begin{comment}
\begin{enumerate}
\item %\carlos{how many points in each cluster?}
For the dataset in Figure
~\ref{Spectral-figs} assume that the first cluster has $m_1$ points and the second one has $m_2$ points. If
we use equation ~\ref{aff-eqn} to compute affinity matrix $A$, is there a value of $\Theta$
for which you can analytically compute the first two eigenvalues and  eigenvectors? If not,
explain why not. If yes, what are they?. What are the other eigenvalues ?
\item
As in Figure ~\ref{blockfig} we can now compute the spectral embedding of the datapoints using the $k$ top
eigenvectors. %$Y(i,j)=U(i,j)\ j\in\{1 ... k\}$ % <ajc> I think you want a comma before "j\in..." because it all runs together.  What are Y and U?  
For the dataset in figure ~\ref{Spectral-figs}
write down your best guess for the coordinates of the $k=2$ cluster centers.

\item
%However you can only have a block diagonal structure
%if the dataset is already clustered.
% <ajc> This sentence is not grammatical.
A desirable property for any clustering algorithm is that its output should 
be invariant with respect to the ordering of the datapoints.
Let $A$ be the affinity matrix constructed using the ordering $x_1,\dots,x_m$.
And let $B$ be the affinity matrix constructed using the ordering $x_{\pi(1)},\dots,x_{\pi(m)}$
where $\pi$ is a permutation of $\{1,\dots,m\}$.  %This gives a permutated version of the block-diagonal matrix in figure ~\ref{block1}.

A permutation matrix $P$ is a matrix obtained by permuting the columns
of an $m\times m$ identity matrix according to some permutation of
the numbers $1$ to $m$. Every row and column therefore contains
precisely a single 1 with 0s everywhere else, and every permutation
corresponds to a unique permutation matrix.  For example, one possible
permutation matrix is:
\[
P = \left[
\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 \\
\end{array}\right]
\]

\begin{enumerate}
\item
Show that there exists a permutation matrix $P$ such that $AP=PB$.
\textit{Hint: Work a few examples to see how left multiplication by a permutation matrix compares
to right multiplication by a permutation matrix.}
\item Show that $P^T P=I$.
\end{enumerate}


\item

%Now going back to the ideal case where $A$ is block-diagonal, prove that
%(\carlos{Who can see? Are you asking them to prove this?})
%if $B$ is the affinity matrix constructed from a permuted set of points,
%then it will have the same eigenvectors (up to permutation).
Show that $A$ and $B$ have the same eigenvectors up to a permutation of the coordinates.
{\em Hint: Use the Singular Value Decomposition and the fact that both $A$ and $B$ are 
symmetric.
}
\end{enumerate}
\end{comment}

\subsection*{Algorithm description}

%\carlos{I am assuming the datasets for this question are in the class
%webpage.  You should note this...}
Frequently, the affinity matrix is constructed as
\begin{equation}
\label{aff-eqn2}
A_{ij} =\begin{cases}
1 & \mbox{If $i,j$ are amongst $k$ nearest neighbors of each other}\\
0 & \mbox{Otherwise}
\end{cases} 
\end{equation}
The best that we can hope for in practice is a
near block-diagonal affinity matrix.  It can be shown
in this case, that after projecting to the space spanned by the top $k$
eigenvectors, points which belong to the same block
are close to each other in a euclidean sense.  We won't try to prove this, but
using this intuition, you will implement one (of many) possible
spectral clustering algorithms.  This particular algorithm is described in
\begin{verbatim}
On Spectral Clustering: Analysis and an algorithm
Andrew Y. Ng, Michael I. Jordan, Yair Weiss (2001)
\end{verbatim}
We won't try to justify every step, but see the paper if you are interested.
The steps are as follows:
\begin{itemize}
\item Construct an affinity matrix $A$ using Equation ~\ref{aff-eqn2}. 
\item Symmetrically `normalize' the rows and columns of $A$ to get a matrix $N$:
such that $N(i,j)=\frac{A(i,j)}{\sqrt{d(i)d(j)}}$, where $d(i)=\sum_k A(i,k)$.
\item Construct a matrix $Y$ whose columns are the first $k$ eigenvectors of $N$.
\item Normalize each row of $Y$ such that it is of unit length.
\item Cluster the dataset by running $k$-means on the set of spectrally embedded points, where each row of $Y$ is a datapoint.

%\carlos{this part needs clarification. What are the datapoints that you give to
%k-means?}
\end{itemize}

%\begin{figure}[ht!]
 % \centering
  %\subfigure{
   % \includegraphics[width=2.5in]{concentric.eps}
  %}
  %\subfigure{
   % \includegraphics[width=2.5in]{rectangles.eps}
  %}
%\subfigure{
 %   \includegraphics[width=2.5in]{links.eps}
 % }

  %\caption{Synthetic Datasets.}
%\end{figure}

\begin{enumerate}
	\item Generate a dataset with 10000 points, with 5000 coming from a Gaussian centered at $\mu_1(i)=3/\sqrt{p}$  with $\Sigma_1=I_p$ and the rest from a mean $\mu_2(i)=-3/\sqrt{p}$, for $i=1,\dots,p$, gaussian with $\Sigma_2=I_p$. Create a k-nearest neighbor graph from this dataset and do Spectral clustering. For $p\in\{250,500,1000,2000\}$, plot the time taken and the clustering accuracy of the following algorithms, averaged over 10 randomly generated datasets. You just have to write your own code for Spectral clustering, you can use available software like E2LSH or packages for kdtree for the rest. There are four algorithms, \textit{Exact}, \textit{JL+Exact}, \textit{JL+KDtrees} and \textit{E2LSH}. Use $k=5$. You can also try out different $k$, its interesting to try this because too small or too large $k$ can lead to a bad clustering accuracy. Remember that you can calculate the clustering accuracy because you generated the data and hence know the ``latent'' ground truth memberships.
\begin{enumerate}
	\item (Exact) Use the brute force k-nearest neighbor algorithm. If this is taking too long, you may omit the results, but please write explicitly how long it took, e.g. ``my k-nn graph took over yyy hours to build when $p=xxx$ and so I gave up.''
	\item (JL+Exact, JL+KDtrees) Use the Johnson Lindenstrauss lemma to reduce the dimensionality of the data. Remember, for $n$ points JL lets you project the datapoints into a $O(\log n/\epsilon^2)$ dimensional space with a multiplicative distortion of $\epsilon$ of the distances. Try out different $\epsilon$ values to generate the curves. For small $\epsilon$, you would have higher accuracy and longer processing time, whereas for larger $\epsilon$ you would have lower accuracy and less computation time. Now use the KDtree algorithm and exact k-nearest neighbors to build the knn tree. There is a builtin function in Matlab using knnsearch which has an option of using the KDtree.
	\item (E2LSH) Use E2LSH to implement Locality sensitive hashing to obtain the nearest neighbor graph. You can find a matlab package here:
	\url{http://ttic.uchicago.edu/~gregory/download.html}.
	\end{enumerate}
%For text.mat, take $k=6$.  
\end{enumerate}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\ba}{\boldsymbol{a}}

\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\bo}{\boldsymbol{1}}

\item We are going to learn and implement the power method in this problem. Let the eigenvalues of a square symmetric matrix $A\in \mathbb{R}^{n\times n}$ be given by $|\lambda_1|>|\lambda_2| > \dots$. We will assume that the eigenvalues are all different for convenience of analysis. Start with some vector $\bq^{(0)}$. Now compute the following:
\begin{align}
\bz^{(k)}&=A\bq^{(k-1)}\\
\bq^{(k)}&=\frac{\bz^{(k-1)}}{\|\bz^{(k-1)}\|}\\
\nu^{(k)}&=(\bq^{(k)})^TA\bq^{(k)}
\end{align}
\begin{enumerate}
%\begin{align*}
\item Prove that for $k>1$, $$\bq^{(k)}=\frac{A^k \bq^{(0)}}{\|A^k \bq^{(0)}\|}.$$ You can use induction to do this.
\item Now let the eigenvectors of $A$ be $\bx_1,\dots,\bx_n$ and let $$\bq^{(0)}=\sum_{i=1}^n\alpha_i\bx_i,\ \ \ \by^{(k)}=\sum_{i=2}^n \frac{\alpha_i}{\alpha_1} \left(\frac{\lambda_i}{\lambda_1}\right)^k\bx_i.$$
Show that \begin{align*}
\|\bq^{(k)}-\langle\bq^{(k)},\bx_1\rangle \bx_1\|\leq C\left|\frac{\lambda_2}{\lambda_1}\right|^k
\end{align*}
%\end{align*}
\end{enumerate}
\item Now, write your own routine for power iteration and apply it to compute the second eigenvector of the nearest neighbor graphs you have built in the last question. For each of them, show the error (you should adjust for signs since an eigenvector can be multiplied by 1 or -1 but it still remains an eigenvector) of your estimated vector and the second eigenvector computed using matlab or R.
\item Last, but not the least, we will learn kernel PCA to deal with non-linear decision boundaries.
Recall PCA? There you first centered your data to get $\tilde{X}$, computed covariance matrix, and then compute top $K$ eigenvectors $V_k$ of this and project a datapoint $\tilde{x}$ to get $\tilde{x}^TV_k$. Now we will directly compute these projections .
\begin{enumerate}
\item Assume that you are mapping the datapoints to a different feature space $\phi(\bx)\in \mathbb{R}^N$ and $\sum_i \phi(\bx_i)=0$.
While we will not do this explicitly, let us follow through the steps of PCA applied on this new feature space. Create a new matrix $\phi(X)\in\mathbb{R}^{n\times N}$. Let $\bv$ be the first eigenvector of the covariance matrix in this feature space, i.e.
\begin{align}
\label{eq:pca}
\sum_{i=1}^n\phi(\bx_i)\phi(\bx_i)^T\bv=\lambda \bv.
\end{align}
 Show that $\bv$ can be written as 
\begin{align}\label{eq:bv}
\bv=\phi(X)^T\ba
\end{align}
So finding $\bv$ boils down to finding $\ba$. 
\item Now show that if you could get $\ba$, the projection of the data (in the new space) on this direction is given by:
$$\Phi(X)\bv=K\ba,$$
where $K(i,j)=\phi(\bx_i)^T\phi(\bx_j)$. Now we will get $\ba$.
\item Now plug in Eq~\eqref{eq:bv} to Eq~\eqref{eq:pca}, and left multiply by $\phi(\bx_k)^T$ to get:
$$K\ba=m\lambda \ba.$$
You can assume that $K$ is invertible.
\item But typically we don't have centered features. So instead of $\phi(\bx_i)$ we should work with $\psi(\bx_i)=\phi(\bx_i)-\dfrac{\sum_j \phi(\bx_j)}{n}$
Show that the kernel matrix built from these centered feature vectors equal $\tilde{K}=K-K\bo\bo^T/n-\bo\bo^T/nK+\bo^TK\bo/n^2\bo\bo^T$
\item So the final algorithm is to build $\tilde{K}$ matrix from the data using your choice of a kernel, and then compute top eigenvector of this matrix and project $K$ on that direction. Download the two parabola dataset. Plot the first principal component using PCA. 
\item Now do Kernel PCA with the RBF kernel $K(i,j)=\exp(-\|\bx_i-\bx_j\|^2/2\sigma^2)$. Change your power method algorithm so that you do not need to compute the $\tilde{K}$ matrix, but only the $K$ matrix. Provide the Pseudocode.
\item Try different values of $\sigma$, and report the one which returns a first kernel PC such that the two classes are separated along this direction.
\item What do you think is the relationship of Spectral Clustering with Kernel PCA? Give your answer in 4-5 lines.
\end{enumerate}
\end{enumerate}
\end{document}














