\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,verbatim}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ib}{\end{enumerate}}
\newcommand{\p}{\item}
	

\begin{document}

\title{{\bf Homework Assignment 1}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
Please upload the HW on canvas before class Oct 11th by 10am. Please type up your homework using latex. We will not accept handwritten homeworks. 
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\newpage
\item (10 pts) \textbf{Convex functions: } Using the definition of convex function, i.e. $f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)$ show that the following functions are convex.
\bi
\p (3pts) $e^x$
\p (2pts) If $f(x)$ is convex for $x\in\Re^p$, show that so is $f(Ax+b)$ for $A\in \Re^{p\times p}$ and $b\in \Re^p$.
\p (2pts) If $f_i(x),i\in[k]$ are convex functions, show that the pointwise maximum, i.e. $g(x)=\max_{i\in[k]}f_i(x)$ is also convex.
\p (3 pts) Consider the logistic regression problem. For $x\in \Re^p$, You have
$$y_i=\frac{1}{1+e^{-\theta^T x}}$$

\bi
\p (1pt) Write down the log likelihood function.
\p (2pt) Show that this is concave.
\textit{Hint: In class we showed that the $\log(1+e^x)$ function is convex when the argument is a scalar. You will have to extend that to allow multivariate arguments.}
\ib

\ib
\item (10 pts) \textbf{Convergence of gradient descent:} In class, we used strong convexity to show convergence of GD. In this homework we will revisit this for Lipschitz functions. To be concrete, suppose the function $f$ is convex and differentiable and its gradient is Lipschitz condition with constant $L>0$, i.e. we have 
$$\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|_2, \qquad \mbox{For any $x,y$}$$
In this problem we run GD for $k$ iterations with a fixed step size $t<1/L$.
\begin{enumerate} 
	\item (1 pt) First show that for any $y$,
	$$f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac{L}{2}\|y-x\|^2$$
	\item (3 pts) Let $y'=x-t\nabla f(x)$. Now show:
	$$f(y')\leq f(x)-t\|\nabla f(x)\|^2/2$$
	\item (3 pts) Now show that $f(y')-f(x^*)\leq \frac{1}{2t}(\|x-x^*\|^2-\|y'-x^*\|^2)$
	\item (3 pts) Using this, show that $$f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}$$
\end{enumerate}

\item (20 pts) \textbf{Programming question} 
Logistic regression is a simple statistical classification method which models the conditional
distribution of the class variable y being equal to class c given an input $x\in \mathbb{R}^p$.
 We will  examine two classification tasks, one classifying newsgroup posts, and the other classifying digits. In these tasks the input x is some description of the sample (e.g. word counts in the news case) and y is the category the sample belongs to (e.g. sports, politics). The Logistic Regression model assumes the class distribution conditioned on x is log-linear. For $C$ classes, the goal is to learn $\beta_1,\dots \beta_{C-1}\in \mathbb{R}^p$. We use the $K^{th}$ class as a pivot.
 \begin{align*}
 \log \frac{p(Y=1|X=x;\beta_{1},\dots,\beta_{C-1})}{p(Y=C|X=x;\beta_{1},\dots,\beta_{C-1})}=\beta_1^T x
 %P(Y=y|X=x,\beta_{1},\dots,\beta_{C})=\frac{\exp(-\beta_c^T x)}{\sum_{j=1}^C\exp(-\beta_j^Tx)}
 \end{align*}
 Another way to think about this is to take $\beta_C$ as all zeros. Thus,
 \begin{align}\label{eq:posterior}
 P(Y=c|X=x,\beta_{1},\dots,\beta_{C})=\frac{\exp(-\beta_c^T x)}{\sum_{j=1}^C\exp(-\beta_j^Tx)}.
 \end{align}
 Once the model is learned, one can classify a new point by picking the class that maximizes the posterior probability of belonging to that class (see Eq~\ref{eq:posterior}). You can measure convergence by the relative error of the concantenated parameter vector $\beta=[\beta_1^T \dots \beta_{K-1}^T]\in\mathbb{R}^{p(C-1)}$. You should write your loss function as an average, and you can use the regularization parameter to be $1/n$.
 \bi
 \p Write down the likelihood of this model for $n$ datapoints.
 \p Is this concave? Why?
 \p For the two datasets in the provided zip file, implement the following four methods. You will use $\ell_2$ regularization.
\bi
\p Gradient descent
\p Newton Raphson
\p The heavy ball momentum method you learned in class
\p Stochastic gradient descent
\p Minibatch Stochastic gradient descent
\ib
\p For each method, plot the loglikelihood as a function of number of iterations.
\p For gradient descent  try different step-sizes and provide a discussion on the effect of stepsize on the convergence.
\p Try different batch-sizes for batch gradient descent and discuss the effect of batchsize on convergence.
\p For SGD and minibatch SGD, how are you choosing your step-size? 
Show a plot with decreasing step-size and for fixed step-size.
\p Provide a short discussion on your choice of the hyperparameters of the heavy ball method and how it affects the convergence.
\p Finally compute the test set error and compare the 5 methods on both of the datasets.
 \ib
\end{enumerate}
%\end{comment}



\end{document} 
