\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,verbatim}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ib}{\end{enumerate}}
\newcommand{\p}{\item}
	

\begin{document}

\title{{\bf Homework Assignment 1}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
Please upload the HW on canvas before class Oct 2nd by 10am. Please type up your homework using latex. We will not accept handwritten homeworks. Each group should write the names and EID's of the members clearly on the submission and there should be one submission for each group. 
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\newpage
\item (10 pts) \textbf{Convex functions: } Using the definition of convex function, i.e. $f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)$ show that the following functions are convex.
\bi
\p (3pts) $e^x$
\p (2pts) If $f(x)$ is convex for $x\in\Re^p$, show that so is $f(Ax+b)$ for $A\in \Re^{p\times p}$ and $b\in \Re^p$.
\p (2pts) If $f_i(x),i\in[k]$ are convex functions, show that the pointwise maximum, i.e. $g(x)=\max_{i\in[k]}f_i(x)$ is also convex.
\p (3 pts) Consider the logistic regression problem. For $x\in \Re^p$, You have
$$y_i=\frac{1}{1+e^{-\theta^T x}}$$

\bi
\p (1pt) Write down the log likelihood function.
\p (2pt) Show that this is concave.
\textit{Hint: In class we showed that the $\log(1+e^x)$ function is convex when the argument is a scalar. You will have to extend that to allow multivariate arguments.}
\ib

\ib
\item (10 pts) \textbf{Convergence of gradient descent:} In class, we used strong convexity to show convergence of GD. In this homework we will revisit this for Lipschitz functions. To be concrete, suppose the function $f$ is convex and differentiable and its gradient is Lipschitz condition with constant $L>0$, i.e. we have 
$$\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|_2, \qquad \mbox{For any $x,y$}$$
In this problem we run GD for $k$ iterations with a fixed step size $t<1/L$.
\begin{enumerate} 
	\item (1 pt) First show that for any $y$,
	$$f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac{L}{2}\|y-x\|^2$$
	\item (3 pts) Let $y'=x-t\nabla f(x)$. Now show:
	$$f(y')\leq f(x)-t\|\nabla f(x)\|^2/2$$
	\item (3 pts) Now show that $f(y')-f(x^*)\leq \frac{1}{2t}(\|x-x^*\|^2-\|y'-x^*\|^2)$
	\item (3 pts) Using this, show that $$f(x^{(k)})-f(x^*)\leq \frac{\|x^{(0)}-x^*\|^2}{2tk}$$
\end{enumerate}

\item (20 pts) \textbf{Programming question} Read the paper ``Large Scale Online Learning'' (LSOL) in \url{http://yann.lecun.com/exdb/publis/pdf/bottou-lecun-04b.pdf} by Bottou and Le Cun et al. Follow the data generation procedure sketched in Section 5. In particular, draw two separate sets for training and testing with $10^5$ examples each. Do \textit{not} add the column of all ones as specified in the LSOL paper. Now generate 30 permutations of the first set. Train each learning algorithm (given below)  using various number of examples taken sequentially from the beginning of the permuted sets. Measure the resulting performance  on the testing set and average over the 30 permutations. You can compute your ground truth using the test set and you can use the initialization of $Uniform(-1/2,1/2)$ for the parameter vector. You can use the identity matrix as the common  covariance matrix of all classes.
\bi
\p (5 pts) Implement the batch newton algorithm  with the Gauss-Newton approximation sketched in ``Efficient Back-prop'' by Le Cun, Y., Bottou, L., Orr, G. B., and Muller, K.-R in 1998.
\p (5 pts) Implement the Online-Kalman algorithm as sketched in the LSOL paper.
\p (5 pts) Reproduce Figures 1,2, 3, and 4 in the LSOL paper. 
\p (5 pts) Write a one page discussion of your findings and how that aligns with the methodological and theoretical results shown in this paper.
\ib
\end{enumerate}
%\end{comment}



\end{document} 
