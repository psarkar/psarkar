\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,verbatim}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ib}{\end{enumerate}}
\newcommand{\p}{\item}
	

\begin{document}

\title{{\bf Homework Assignment 1}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
Please upload the HW on canvas before class Oct 11th by 10am. Please type up your homework using latex. We will not accept handwritten homeworks. 
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\newpage
\item (10 pts) \textbf{Convex functions: } Using the definition of convex function, i.e. $f(tx+(1-t)y)\leq tf(x)+(1-t)f(\beta')$ show that the following functions are convex.
\bi
\p (3pts) $e^x$
\p (2pts) If $f(x)$ is convex for $x\in\Re^p$, show that so is $f(Ax+b)$ for $A\in \Re^{p\times p}$ and $b\in \Re^p$.
\p (2pts) If $f_i(x),i\in[k]$ are convex functions, show that the pointwise maximum, i.e. $g(x)=\max_{i\in[k]}f_i(x)$ is also convex.
\p (3 pts) Consider the logistic regression problem. For $x\in \Re^p$, You have
$$y\sim Bernoulli\left(\frac{1}{1+e^{-\theta^T x}}\right)$$

\bi
\p (1pt) Write down the log likelihood function.
\p (2pt) Show that this is concave.
\textit{Hint: for part d you can use first/second order conditions and properties of concave functions.}
\ib

\ib
\item (10 pts) \textbf{Convergence of gradient descent:} In class, we used strong convexity to show convergence of GD. In this homework we will revisit this for Lipschitz functions. To be concrete, suppose the function $f$ is convex and differentiable and its gradient is Lipschitz condition with constant $L>0$, i.e. we have 
$$\|\nabla f(\beta)-\nabla f(\beta')\|\leq L\|\beta-\beta'\|_2, \qquad \mbox{For any $\beta,\beta'$}$$
In this problem we run GD for $t$ iterations with a fixed step size $\alpha<1/L$.
\begin{enumerate} 
	\item (1 pt) First show that for any $\beta'$,
	$$f(\beta')\leq f(\beta)+\nabla f(\beta)^T(\beta'-\beta)+\frac{L}{2}\|\beta'-\beta\|^2$$
	\item (3 pts) Let $\beta_{t+1}=\beta_t-\alpha\nabla f(\beta_t)$. Now show:
	$$f(\beta_{t+1})\leq f(\beta_t)-\alpha\|\nabla f(\beta_t)\|^2/2$$
	\item (3 pts) Now show that $f(\beta_{t+1})-f(\beta^*)\leq \frac{1}{2\alpha}(\|\beta_t-\beta^*\|^2-\|\beta_{t+1}-\beta^*\|^2)$
	\item (3 pts) Using this, show that $$f(\beta_t)-f(\beta^*)\leq \frac{\|\beta_0-\beta^*\|^2}{2\alpha t}$$
\end{enumerate}
%\item (Extra credit: 10 pts) \textbf{Convergence of stochastic gradient descent:} Using the same steps as before, show that when the gradients have bounded variates, i.e. $E[\|\nabla f(x_{\sigma_t},\beta)-\nabla f(X;\beta)\|^2]\leq \sigma^2$ for all $\beta$. Then for any $t> 1$ and $\alpha<1/L$, we have:
%\begin{align*}
%E[f\left(\frac{1}{t}\sum_{i=1}^t \beta_i\right)]\leq f(\beta^*)+\frac{}{}
%\end{align*}

\item (20 pts) \textbf{Programming question} 
Logistic regression is a simple statistical classification method which models the conditional
distribution of the class variable y being equal to class c given an input $x\in \mathbb{R}^p$.
 We will  examine two classification tasks, one classifying newsgroup posts, and the other classifying digits. In these tasks the input x is some description of the sample (e.g. word counts in the news case) and y is the category the sample belongs to (e.g. sports, politics). The Logistic Regression model assumes the class distribution conditioned on x is log-linear. For $C$ classes, the goal is to learn $\beta_1,\dots \beta_{C-1}\in \mathbb{R}^p$. We use the $K^{th}$ class as a pivot.
 \begin{align*}
 \log \frac{p(Y=1|X=x;\beta_{1},\dots,\beta_{C-1})}{p(Y=C|X=x;\beta_{1},\dots,\beta_{C-1})}=\beta_1^T x
 %P(Y=y|X=x,\beta_{1},\dots,\beta_{C})=\frac{\exp(-\beta_c^T x)}{\sum_{j=1}^C\exp(-\beta_j^Tx)}
 \end{align*}
 Another way to think about this is to take $\beta_C$ as all zeros. Thus,
 \begin{align}\label{eq:posterior}
 P(Y=c|X=x,\beta_{1},\dots,\beta_{C})=\frac{\exp(\beta_c^T x)}{\sum_{j=1}^C\exp(\beta_j^Tx)}.
 \end{align}
 Once the model is learned, one can classify a new point by picking the class that maximizes the posterior probability of belonging to that class (see Eq~\ref{eq:posterior}). You can measure convergence by the relative error of the concantenated parameter vector $\beta=[\beta_1^T \dots \beta_{K-1}^T]\in\mathbb{R}^{p(C-1)}$. You should write your loss function as an average, and you can use the regularization parameter to be $1/n$.
 \bi
 \p Write down the likelihood of this model for $n$ datapoints.
 \p [Extra credit] Is the logarithm of this concave? Why? 
 \p For the two datasets in the provided zip file, implement the following four methods. You will use $\ell_2$ regularization.
\bi
\p Gradient descent
\p Stochastic gradient descent
\p Newton Raphson
\ib
\p For each method, plot the loglikelihood as a function of number of iterations.
\p For gradient descent  try different step-sizes and provide a discussion on the effect of stepsize on the convergence.
\p For SGD , how are you choosing your step-size? 
\p Finally compute the test set error and compare the GD and SGD methods on both of the datasets.
\p Show the test error of NR on the small dataset. How does it perform compared to the other algorithms on the small dataset?
 \ib
\end{enumerate}
%\end{comment}



\end{document} 
