\documentclass[11pt]{article}
\usepackage[left=1in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,bm}
\usepackage{amsbsy,verbatim}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\DeclareMathOperator*{\argmin}{\arg\!\min}



\begin{document}

\title{{\bf Homework Assignment 3\\
Due by end of the day Dec 5th via canvas}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
%\textbf{Set algebra and probability laws.}
\begin{enumerate}
\item \textbf{Theory of LSH}
Recall that a locality sensitive hashing scheme is a set $\mathcal{F}$ of hashing functions that operate on a set $S$ of objects, such that for two objects $x, y \in S$,
$$P(h(x)=h(y))=sim(x,y),$$
where $sim(x,y):	S\times S\rightarrow [0,1]$ is a similarity function.
\begin{enumerate}
	\item Let $d(x,y)=1-sim(x,y)$. Prove that sim(.) to have a locality sensitive hashing scheme, $d(x,y)$ should satisfy the triangle inequality.
	$$d(x,y)+d(y,z)\geq d(x,z)\ \ \ \ \mbox{for all }x,y,z\in S$$
	\item Consider the following two similarity functions for two sets $A$ and $B$. The overlap similarity function:
	$$sim_{over}(A,B)=\frac{|A\cap B|}{\min(|A|,|B|)}$$
	and the Dice similarity function
	$$sim_{dice}(A,B)=\frac{2|A\cap B|}{|A|+|B|}$$
	Is there a locality sensitive scheme for these? Prove or give a counter example.
	%\item In class we considered the Jaccard score for sets. Every element appeared exactly once in a set, i.e. we did not consider counts of elements. Consider a multiset, which is allowed to have multiple instances of an element. In this setting we use the weighted Jaccard:
%	$$J(A,B)=\frac{\sum_i \min(x_i,y_i)}{\sum_i \max(x_i,y_i)}$$	where $x_i$ denotes the number of times element $i$ occurs in set $A$ and $y_i$ denotes the number of times element $i$ occurs in set $B$. Prove that the minhashing scheme works here as well, i.e. $$
\item Sometimes it is more convenient to have a hash function family that maps objects to $\{0, 1\}$. In that case, the output of $T$ different hash functions can simply be concatenated to obtain a $T$-bit binary hash string for an object. Here you will show that we can always obtain such a binary hash function family with a slight change in the similarity measure. Given a locality sensitive hash function family $\mathcal{F}$ corresponding to a similarity function $sim(x,y)$, give a mechanism to obtain a locality sensitive hash function family $\mathcal{F}'$ that maps
objects to $\{0, 1\}$ and corresponds to the similarity function $\frac{1+sim(x,y)}{2}$ .
\end{enumerate} 
\item \textbf{Spectral Clustering\footnote{This problem was designed in collaboration with Jon Huang}}
In class we have seen k-means and EM for estimating GMM's. Since your professor will not get a chance to actually cover her favorite clustering method aka spectral clustering, methinks this is a great opportunity to introduce you to it.
There is a class of clustering algorithms, called 
spectral clustering algorithms, which has recently 
become quite popular.  Many of these algorithms are 
quite easy to implement and perform well on 
certain clustering problems compared to more traditional 
methods like $k$-means.  In this problem, we will try to 
develop some intuition about why these approaches
make sense and implement one of these algorithms.

Before beginning, we'll review a few basic linear
algebra concepts you may find useful for some
of the problems.
\begin{itemize}
\item If $A$ is a matrix, it has an 
$v$ with eigenvalue $\lambda$ if $Av=\lambda v$.
\item For any $m\times m$ symmetric matrix $A$, 
the \emph{Singular Value Decomposition} of $A$ yields a 
factorization of $A$ into
\[
A = USU^T
\]
where U is an $m\times m$ orthogonal matrix 
(meaning that the columns are pairwise orthogonal).
and $S=diag(|\lambda_1|,|\lambda_2|,\dots,|\lambda_m|)$ 
where the $\lambda_i$ are the eigenvalues of $A$. 
\end{itemize}

Given a set of $m$ datapoints $x_1,\dots,x_m$,
the input to a spectral clustering algorithm typically
consists of a matrix, $A$, of pairwise similarities between
datapoints often called the \emph{affinity matrix}.  The choice of
how to measure similarity between points is one which is often left 
to the practitioner.  A very simple affinity matrix can be
constructed as follows:
\begin{equation}
\label{aff-eqn}
A(i,j)=A(j,i)=\left\{ \begin{array}{cc} 1 & \mbox{if $d(x_i,x_j)<\Theta$} \\
0 & \mbox{otherwise} \\
\end{array}\right.
\end{equation}
where $d(x_i,x_j)$ denotes the Euclidean distance 
between points $x_i$ and $x_j$.

The general idea of spectral clustering is to construct
a mapping of the datapoints to an eigenspace of $A$ with the 
hope that points are well separated in this eigenspace so that something
simple like $k$-means applied to these new points will perform well.
\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=2.0in]{fourdots.eps}\\
\end{center}
\caption{\label{fourdots-fig} Simple dataset}
\end{figure}

As an example, consider forming the affinity matrix for 
the dataset in Figure ~\ref{fourdots-fig} using Equation ~\ref{aff-eqn} with
$\Theta=1$.  We have that
\[
A = \left[\begin{array}{cccccc}
 & \vline & a & b & c & d \\
 \hline
a & \vline & 1 & 1 & 0 & 0 \\
b & \vline & 1 & 1 & 0 & 0\\
c & \vline & 0 & 0 & 1 & 1\\
d & \vline & 0 & 0 & 1 & 1\\
\end{array}\right]
\]

Now for this particular example, the clusters $\{a,b\}$ and $\{c,d\}$
show up as nonzero blocks in the affinity matrix.  This is, of course,
artificial, since we could have constructed the matrix $A$ using any
ordering of $\{a,b,c,d\}$.  For example, another possibile affinity matrix
for $A$ could have been:
\[
\tilde{A} = \left[\begin{array}{cccccc}
 & \vline & a & c & b & d \\
 \hline
a & \vline & 1 & 0 & 1 & 0 \\
c & \vline & 0 & 1 & 0 & 1\\
b & \vline & 1 & 0 & 1 & 0\\
d & \vline & 0 & 1 & 0 & 1\\
\end{array}\right]
\]

The key insight here is that the eigenvectors of 
matrices $A$ and $\tilde{A}$ have the same entries (just 
permuted).  The eigenvectors with nonzero eigenvalue of $A$
are: $e_1=(.7,.7,0,0)^T$, $e_2=(0,0,.7,.7)$.  And the nonzero 
eigenvectors of $\tilde{A}$ are: $\tilde{e}_1=(.7,0,.7,0)^T$, $\tilde{e}_2=(0,.7,0,.7)$.
Spectral clustering embeds the original datapoints in a new space
by using the coordinates of these eigenvectors.  Specifically, it maps the 
point $x_i$ to the point $(e_1(i),e_2(i),\dots,e_k(i))$ where $e_1,\dots,e_k$ 
are the top $k$ eigenvectors of $A$.  We refer to this mapping as the \emph{spectral embedding}.  
%See Figure ~\ref{blockfig} for an example.

%\begin{figure}[ht!]
 % \label{blockfig}
  %\begin{center}
   % \includegraphics[width=3in]{block2.eps}\\
%\end{center}
%\caption{Using eigenvectors to embed the datapoints.  Notice that the points $\{a,b,c,d\}$ are tightly 
%clustered in this space}
%	\end{figure}

%\begin{figure}
 % \begin{center}
  %  \includegraphics[width=3.2in]{purnafig.eps}\\
%\end{center}
%\caption{\label{Spectral-figs}Dataset with rectangles}
%\end{figure}

\begin{comment}
\begin{enumerate}
\item %\carlos{how many points in each cluster?}
For the dataset in Figure
~\ref{Spectral-figs} assume that the first cluster has $m_1$ points and the second one has $m_2$ points. If
we use equation ~\ref{aff-eqn} to compute affinity matrix $A$, is there a value of $\Theta$
for which you can analytically compute the first two eigenvalues and  eigenvectors? If not,
explain why not. If yes, what are they?. What are the other eigenvalues ?
\item
As in Figure ~\ref{blockfig} we can now compute the spectral embedding of the datapoints using the $k$ top
eigenvectors. %$Y(i,j)=U(i,j)\ j\in\{1 ... k\}$ % <ajc> I think you want a comma before "j\in..." because it all runs together.  What are Y and U?  
For the dataset in figure ~\ref{Spectral-figs}
write down your best guess for the coordinates of the $k=2$ cluster centers.

\item
%However you can only have a block diagonal structure
%if the dataset is already clustered.
% <ajc> This sentence is not grammatical.
A desirable property for any clustering algorithm is that its output should 
be invariant with respect to the ordering of the datapoints.
Let $A$ be the affinity matrix constructed using the ordering $x_1,\dots,x_m$.
And let $B$ be the affinity matrix constructed using the ordering $x_{\pi(1)},\dots,x_{\pi(m)}$
where $\pi$ is a permutation of $\{1,\dots,m\}$.  %This gives a permutated version of the block-diagonal matrix in figure ~\ref{block1}.

A permutation matrix $P$ is a matrix obtained by permuting the columns
of an $m\times m$ identity matrix according to some permutation of
the numbers $1$ to $m$. Every row and column therefore contains
precisely a single 1 with 0s everywhere else, and every permutation
corresponds to a unique permutation matrix.  For example, one possible
permutation matrix is:
\[
P = \left[
\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 \\
\end{array}\right]
\]

\begin{enumerate}
\item
Show that there exists a permutation matrix $P$ such that $AP=PB$.
\textit{Hint: Work a few examples to see how left multiplication by a permutation matrix compares
to right multiplication by a permutation matrix.}
\item Show that $P^T P=I$.
\end{enumerate}


\item

%Now going back to the ideal case where $A$ is block-diagonal, prove that
%(\carlos{Who can see? Are you asking them to prove this?})
%if $B$ is the affinity matrix constructed from a permuted set of points,
%then it will have the same eigenvectors (up to permutation).
Show that $A$ and $B$ have the same eigenvectors up to a permutation of the coordinates.
{\em Hint: Use the Singular Value Decomposition and the fact that both $A$ and $B$ are 
symmetric.
}
\end{enumerate}
\end{comment}

\subsection*{Algorithm description}

%\carlos{I am assuming the datasets for this question are in the class
%webpage.  You should note this...}
Frequently, the affinity matrix is constructed as
\begin{equation}
\label{aff-eqn2}
A_{ij} =\begin{cases}
1 & \mbox{If $i,j$ are amongst $k$ nearest neighbors of each other}\\
0 & \mbox{Otherwise}
\end{cases} 
\end{equation}
The best that we can hope for in practice is a
near block-diagonal affinity matrix.  It can be shown
in this case, that after projecting to the space spanned by the top $k$
eigenvectors, points which belong to the same block
are close to each other in a euclidean sense.  We won't try to prove this, but
using this intuition, you will implement one (of many) possible
spectral clustering algorithms.  This particular algorithm is described in
\begin{verbatim}
On Spectral Clustering: Analysis and an algorithm
Andrew Y. Ng, Michael I. Jordan, Yair Weiss (2001)
\end{verbatim}
We won't try to justify every step, but see the paper if you are interested.
The steps are as follows:
\begin{itemize}
\item Construct an affinity matrix $A$ using Equation ~\ref{aff-eqn2}. 
\item Symmetrically `normalize' the rows and columns of $A$ to get a matrix $N$:
such that $N(i,j)=\frac{A(i,j)}{\sqrt{d(i)d(j)}}$, where $d(i)=\sum_k A(i,k)$.
\item Construct a matrix $Y$ whose columns are the first $k$ eigenvectors of $N$.
\item Normalize each row of $Y$ such that it is of unit length.
\item Cluster the dataset by running $k$-means on the set of spectrally embedded points, where each row of $Y$ is a datapoint.

%\carlos{this part needs clarification. What are the datapoints that you give to
%k-means?}
\end{itemize}

%\begin{figure}[ht!]
 % \centering
  %\subfigure{
   % \includegraphics[width=2.5in]{concentric.eps}
  %}
  %\subfigure{
   % \includegraphics[width=2.5in]{rectangles.eps}
  %}
%\subfigure{
 %   \includegraphics[width=2.5in]{links.eps}
 % }

  %\caption{Synthetic Datasets.}
%\end{figure}

\begin{enumerate}
	\item Generate a dataset with 10000 points, with 5000 coming from a Gaussian centered at $\mu_1=-2/\sqrt{p}$ with $\Sigma_1=I_p$ and the rest from a mean zero gaussian with $\Sigma_2=I_p$. Create a k-nearest neighbor graph from this dataset and do Spectral clustering. For $p\in\{250,500,1000,2000\}$, plot the time taken and the clustering accuracy of the following algorithms, averaged over 10 randomly generated datasets. You just have to write your own code for Spectral clustering, you can use available software like E2LSH or packages for kdtree for the rest. There are four algorithms, \textit{Exact}, \textit{JL+Exact}, \textit{JL+KDtrees} and \textit{E2LSH}. Use $k=5$. You can also try out different $k$, its interesting to try this because too small or too large $k$ can lead to a bad clustering accuracy. Remember that you can calculate the clustering accuracy because you generated the data and hence know the ``latent'' ground truth memberships.
\begin{enumerate}
	\item (Exact) Use the brute force k-nearest neighbor algorithm. If this is taking too long, you may omit the results, but please write explicitly how long it took, e.g. ``my k-nn graph took over yyy hours to build when $p=xxx$ and so I gave up.''
	\item (JL+Exact, JL+KDtrees) Use the Johnson Lindenstrauss lemma to reduce the dimensionality of the data. Remember, for $n$ points JL lets you project the datapoints into a $O(\log n/\epsilon^2)$ dimensional space with a multiplicative distortion of $\epsilon$ of the distances. Try out different $\epsilon$ values to generate the curves. For small $\epsilon$, you would have higher accuracy and longer processing time, whereas for larger $\epsilon$ you would have lower accuracy and less computation time. Now use the KDtree algorithm and exact k-nearest neighbors to build the knn tree. There is a builtin function in Matlab using knnsearch which has an option of using the KDtree.
	\item (E2LSH) Use E2LSH to implement Locality sensitive hashing to obtain the nearest neighbor graph. You can find a matlab package here:
	\url{http://ttic.uchicago.edu/~gregory/download.html}.
	\end{enumerate}
%For text.mat, take $k=6$.  
\end{enumerate}

\end{enumerate}
\end{document}














