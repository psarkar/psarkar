\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb,verbatim}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ib}{\end{enumerate}}
\newcommand{\p}{\item}
	

\begin{document}

\title{{\bf Homework Assignment 2}}
\author{SDS 385 Statistical Models for Big Data}

\date{}

\maketitle{}
Please upload the HW on canvas by 10pm Sunday Oct 21st. Please type up your homework using latex. We will not accept handwritten homeworks. Each group should write the names and EID's of the members clearly on the submission and there should be one submission for each group. 
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\newpage
\item (10 pts) \textbf{Step sizes: } Consider the subgradient method with constant step size $\alpha$, used to minimize the quadratic function $f (x) = (1/2)x^T A x + bT x$, where $B$ is positive definite. For which values of  do we have x(k) → x⋆, for any x(1)? What value of α gives fastest asymptotic convergence?
\bi
\p (5pts) For which values of $\alpha$ do we have $x_k \rightarrow x^*$ for any $x_0$?
\p (5pts) What value of $\alpha$ gives fastest convergence?
\ib
\p (10 pts) Consider the subgradient iteration: $x_{t+1}=x_t-\alpha g$ where $g\in\partial f(x_t)$.
\bi
\p (4pt) Give an example to show that subgradient method is not necessarily a descent method, i.e. moving along the negative subgradient (for minimizing a convex function) does not necessarily reduce the function value, unlike gradient descent.
\p (6pt) Show that if $\alpha< \frac{2(f(x)-f(x_*)}{\|g\|^2}$, we have  $$\|x_{t+1}-x_*\|\leq \|x_t-x_*\|$$
\ib


\item (20 pts) \textbf{Programming question} Consider the Lasso problem of
$$\min_\beta \|y-X\beta\|^2+\lambda\|\beta\|_1$$

Generate $X$ from multivariate mean zero Gaussian with $n=10^5$ and $p=500$ datapoints and your choice of covariance matrix. Choose a ground truth $\beta$ that is sparse with your choice of nonzero entries. Fix the number of nonzero entries at $\lceil p/20 \rceil $.

\bi
\p (5 pts) Implement subgradient descent algorithm with your choice of stepsize. Try a fixed stepsize and a decreasing stepsize. Plot the objective function with growing number of iterations for each algorithm.
\p (5 pts) Implement proximal gradient descent with your choice of stepsize. Plot the objective function with growing number of iterations.
\p (5 pts) Implement proximal gradient descent with backtracking line search. You can find more about backtracking line search in \url{https://web.stanford.edu/class/ee364a/lectures/unconstrained.pdf}. Plot the objective function with growing number of iterations.
\p (5 pts) Now compare these methods with any publicly available software for lasso, e.g. glmfit or lasso or scikit-learn.
\ib
\end{enumerate}
%\end{comment}



\end{document} 
