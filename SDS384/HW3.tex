\documentclass[11pt]{article}
\usepackage[left=1.25in,top=1in,right=1.25in,bottom=1.00in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{epsfig}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{multirow} 
\newcommand{\logit}{\mbox{logit}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{{\small\textsc{HIW}}}
\newcommand{\iw}{{\small\textsc{IW}}}
\newcommand{\N}{\mbox{N}}
\newcommand{\Be}{\mbox{Be}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\C}{\; | \;}
\newcommand{\var}{\text{var}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}




\begin{document}

\title{{\bf Homework Assignment 3}\\Due March 29th by midnight.}
\author{SDS 384-11 Theoretical Statistics}

\date{}

\maketitle{}
%\textbf{Set algebra and probability laws.}
\begin{enumerate}%\item Read Bertsekas and Tsitsiklis, sections 1.1, 1.2 and 1.3.
%\section{Jeffrey's prior}
%\begin{enumerate}
%\end{enumerate}


\item We will use the Efron Stein inequality to obtain bounds of variances for separately convex functions whose partial derivatives exist. A separately convex function $f(x_1,\dots,x_n)$ is a convex function of its $i^{th}$ variable, when all else are held fixed.
\begin{enumerate}
	\item Let $X_1,\dots, X_n$ be independent random variables taking values
in the interval $[0, 1]$ and let $f : [0, 1]^n \rightarrow R$ be a separately convex function whose
partial derivatives exist. Then $f(X):=f(X_1,\dots,X_n)$ satisfies
$$\var(f(X))\leq E[\|\nabla f(X)\|^2]$$
\textit{Hint: Recall that $\var(Z)\leq \sum_i E(Z-E_i Z)^2\leq \sum_i E(Z-Z_i)^2$, where $E_i[Z]=E[Z|X_{1:i-1},X_{i+1:n}]$. Define $Z_i=\inf_{x}f(X_{1:i-1},x,X_{i+1:n})$ and then use convexity of $f$.}
\item Let $A$ be a $m\times n$ random matrix with independent entries $A_{ij}\in[0,1]$. Let 
$$Z=\sqrt{\lambda_1(A^TA)}=\sqrt{\sup_{u\in R^n:\|u\|=1}u^TA^TAu}=\sup_{u\in R^n:\|u\|=1}\|Au\|$$
Show that $\var(Z)\leq 1$.
\end{enumerate}
\item In this question we will look at the Gaussian Lipschitz theorem. Consider $X_1,\dots, X_n\stackrel{iid}{\sim} N(0,1)$
\begin{enumerate}
	\item Prove that the order statistics are 1-Lipschitz.  %of $n$ iid $N(0,1)$ random variables.
	\item Now show that, for large enough $n$, 
	$$c\sqrt{\log n} \leq E[\max_i X_i]\leq \sqrt{2\log n}$$
where $c$ is some universal constant. 
\begin{enumerate}
	\item For the upper bound, let $Y=\max_i X_i$. First show that $\exp(tE[Y])\leq \sum_i E\exp (tX_i)$. Now pick a $t$ to get the right form.
	\item For the lower bound,  do the following steps.
	\begin{enumerate}
		\item Show that $E[Y]\geq \delta P(Y\geq \delta)+E[\min(Y,0)]$
		\item Now show that $E[\min(Y,0)]\geq E[\min(X_1,0)]$
		\item Finally, relate $P(Y\geq \delta)$ to $P(X_1\geq \delta)$ by using independence.
		\item  Now show that $P(X_1\geq \delta)\geq \exp(-\delta^2/\sigma^2)/c$, for some universal constant $c$.
		\item Choose the parameter $\delta$ carefully to have $P(X_1\geq \delta)\geq 1/n$, for large enough $n$. 
	\end{enumerate}
\end{enumerate}
\end{enumerate}
\item In class we proved McDiarmid's inequality for bounded random variables. But now we will look at extensions for unbounded R.V's. Take a look at ``Concentration in unbounded metric spaces and
algorithmic stability'' by Aryeh Kontorovich,   \url{https://arxiv.org/pdf/1309.1007.pdf}. Reproduce the proof of theorem 1. The steps of this proof is very similar to the martingale based inequalities we looked at in class.
\item Consider an i.i.d. sample of size $n$ from a discrete distribution parametrized by $p_1,\dots, p_{m-1}$  on $m$ atoms. A common test for uniformity of the distribution is to look at the fraction of pairs that collide, or are equal. Call this statistic $U$.
\begin{enumerate}
%	\item What is the variance of $U$?
	\item Is $U$ a U statistic? When is it degenerate?
		\item What is the variance of $U$? Please give the exact answer, without approximation. 
	\item For a hypothesis test, we will consider alternative distributions which have $p_i=\frac{1+a}{m}$ for half of the atoms in the distribution and $\frac{1-a}{m}$ for the other half ($0\le a\le 1$), for some $a>0$. Assume that there are an even number of atoms. %Under the alternative, what is the asymptotic distribution of the statistic $U$?
	\begin{enumerate}
		\item What are the mean and variance of this statistic under the null?
		\item What are the mean and variance of this under the alternative?
		\item What is the asymptotic distribution of $U$ under the null hypothesis that $p_i=1/m$? \textit{Hint: you can use the fact that for $X_1,\dots, X_N\stackrel{i.i.d}{\sim} multinomial(q_1,\dots,q_k)$, $\sum_{i=1}^k (N_i-Nq_i)^2/Nq_i\stackrel{d}{\rightarrow} \chi^2_{k-1}$.}
		\item Under the alternative hypothesis,is it always the case that $U$ has a limiting normal distribution? Can you give a sufficient condition on the sample size $n$ so that this is true? 
		%\item Write down the probability of accepting the null hypothesis (which is $p_i=1/m$, for all $i$), when in fact the underlying distribution if coming from the non-uniform distribution parametrized by $a$ described above.
		%\item How big does $n$ and $a$ have to be so that the above probability to be smaller than some small fraction $\delta$? To be concrete, provide a lower bound on $n$ in terms of $m,\ \delta$ and $\epsilon$.
		\end{enumerate}
\end{enumerate}
\end{enumerate}
\end{document} 
