\documentclass[12pt]{article}
\usepackage{natbib,amssymb,amsmath,amsthm,epsfig,color,verbatim}
\usepackage{hyperref}
\setlength{\topmargin}{-.7in}
\setlength{\oddsidemargin}{0.3in}
\setlength{\textwidth}{6.15in}
\setlength{\textheight}{9.2in}
\renewcommand{\baselinestretch}{1.2}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\cov}{\text{cov}}
\newcommand{\bi}{\begin{enumerate}}
	\newcommand{\ib}{\end{enumerate}}
\newcommand{\rd}{\color{red}}
\newcommand{\bk}{\color{black}}
\newcommand{\bl}{\color{blue}}
\newcommand{\p}{\item}
% \pagestyle{empty}
\newcommand{\solution}[1] {\begin{quote}\em \color{blue} {\bf Solution}: #1 \end{quote}}
\newcommand{\grading}[1]{\begin{quote}\footnotesize \bf\em Grading: #1 \end{quote}}
% {#1} {}
%\newcommand{\grading}[1] { }

\newcommand{\Prob}{\mathbf{P}}
\newcommand{\skoo}{\vspace{.2in}}
\newcommand{\var}{\mbox{var}}
\newcommand{\gr}{\color{green}}
\newcommand{\subg}{sub-gaussian\xspace}
\newcommand{\subexp}{sub-exponential\xspace}
\newcommand{\rv}{random variable\xspace}
\newcommand{\rvs}{random variables\xspace}
\newcommand{\cd}{\stackrel{d}{\rightarrow}}
\newcommand{\cp}{\stackrel{P}{\rightarrow}}
\newcommand{\cas}{\stackrel{a.s.}{\rightarrow}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\bR}{\mathbb{R}}
\newcounter{choice}
\renewcommand\thechoice{\Alph{choice}}
\newcommand\choicelabel{\thechoice.}
\newcommand{\Exp}{\mbox{Exponential}}
\newenvironment{choices}%
{\list{\choicelabel}%
	{\usecounter{choice}\def\makelabel##1{\hss\llap{##1}}%
		\settowidth{\leftmargin}{W.\hskip\labelsep\hskip 2.5em}%
		\def\choice{%
			\item
		} % choice
		\labelwidth\leftmargin\advance\labelwidth-\labelsep
		\topsep=0pt
		\partopsep=0pt
	}%
}%
{\endlist}

\newenvironment{oneparchoices}%
{%
	\setcounter{choice}{0}%
	\def\choice{%
		\refstepcounter{choice}%
		\ifnum\value{choice}>1\relax
		\penalty -50\hskip 1em plus 1em\relax
		\fi
		\choicelabel
		\nobreak\enskip
	}% choice
	% If we're continuing the paragraph containing the question,
	% then leave a bit of space before the first choice:
	\ifvmode\else\enskip\fi
	\ignorespaces
}%
{}

\begin{document}
	
	\title{\bf   Final}
	\author{\bf SDS384}
\date{\it Spring 2021} %change for each day.
	\maketitle{}
	
	
	
	\vskip .3cm
%	\noindent You may use two (2 sided) pages of notes, and you may use a calculator.  
	\vskip .3cm
	\noindent This exam has 4 short and 4 long questions. You will have to answer
	 \underline{\textbf{all short questions}}, \underline{\textbf{three long questions}}. The assigned points are noted next to each question; the total number of points is 50.  Please upload your answers in latex by 11:59 pm Sunday May 16th. Use the latex file format provided.
	\vskip .3cm
	\noindent Read each question carefully, \underline{\textbf{show your work}} and \underline{\textbf{clearly present your answers}}. Note, the exam is printed two-sided - please don't forget the problems on the even pages!
	
	\vspace{1cm}
	
	
	
	\vskip 1.5cm
	\begin{center}
		{\Large \bf Good Luck!}
	\end{center}
	\vskip 1.5cm
	
	
	%\noindent{\bf Honor Code Pledge:} ``I pledge my honor that I have not
	%violated the Honor Code during this examination.''
	%\vskip .5cm
	%\hskip 4.5cm Signed: \underline{\hskip 9cm}
	%\vskip 1.2cm
	\noindent{\bf Name: \underline{\hskip 12.3cm}}
	\vskip 1.2cm
	\noindent{\bf UTeid: \underline{\hskip 12.3cm}}
	\vskip 0.5cm
	
	%\noindent{\bf Section: \underline{\hskip 12cm}}
	\newpage
	
	\section{Short questions (17 points)}
	\textbf{Please answer all of the short questions.}
	\begin{enumerate}	
		
%		\rd  The answer is 2k. It is not hard to see that any 2k distinct points on the real line can be shattered using k intervals: it suffices to shatter each of the k pairs of consecutive points with an interval. Assume now that $2k + 1$ distinct points $x_1 < \dots < x_{2k+1}$ are given. For any $i \in  [1,2k+1]$, label $x_i$ with $(−1)^i+1$, that is alternatively label points with 1 or −1. This leads to $k + 1$ points labeled positively and requires $2k + 1$ intervals to shatter the set since no interval can contain two consecutive points. Thus, no set of $2k + 1$ points can be shattered by $k$ intervals and the VC dimension of the union of $k$ intervals is $2k$.\bk
	%	\newpage
	\iffalse
	\item (7 pts) Let $X_1, X_2, \dots , X_n$ be
	i.i.d. samples of random variable with density $f$ on the real line. A standard estimate
	of $f$ is the kernel density estimate 
	\begin{align*}
	\hat{f}(x)=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right)
	\end{align*}
	where $K:\Re\rightarrow [0,\infty)$ is a kernel function satisfying $\int_{-\infty}^\infty K(t)dt=1$, and $h$ is a bandwidth parameter. We will measure the quality of $\hat{f}$ using
	$\|\hat{f}-f\|_1:=\int_{-\infty}^\infty |\hat{f}(t)-f(t)|dt.$
	Prove that:
	$$	P(\|\hat{f}-f\|_1\geq E\|\hat{f}-f\|_1+\delta)\leq e^{-cn\delta^2},$$
	where $c$ is some constant.
\fi
	\item (5 pts) 
Suppose $X_1,\dots,X_n$ are i.i.d random variables with mean $\mu$ and variance $\sigma^2$. Let $T_n=\sum_{j=1}^nz_{nj}X_j$ where $z_{nj}$ are given numbers. Let $\mu_n=E[T_n]$ and $\sigma_n^2=\var(T_n)$. Show that 
\begin{align*}
\frac{T_n-\mu_n}{\sigma_n}\cd N(0,1),
\end{align*}
provided  $\max\limits_{j\leq n}\dfrac{z_{nj}^2}{\sum_{j=1}^nz_{nj}^2}\rightarrow 0$  as  $n\rightarrow\infty$.
\item (5 pts)  Let $X_1,\dots,X_n$ be independent and suppose that $X_n=\sqrt{n}$ with probability $1/2$ and $-\sqrt{n}$ with probability $1/2$, for $n=1,2,\dots$. Find the asymptotic distribution of $\bar{X}_n$. 
	\bk
%%	\newpage

	

%	\newpage
	\item (4 pts)
	Consider a function class with functions of the following form:
	\begin{align}
	f_\alpha(x)=\begin{cases}
	1 & \mbox{If $\sin(\alpha x)>0$}\\
	0&  \mbox{o.w.}\\
	\end{cases}
	\end{align}
	Consider a set of datapoints $\{10^{-i},i=1,\dots,n\}$. Show that any set of labeling $y_i,i=1,\dots, n$ can be achieved by using $$\alpha=\pi\left(1+\sum_{i=1}^n(1-y_i)10^i\right).$$
	Using this, what do you think the VC dimension of this function class is?
\item (3 pts) Consider a r.v. $X$ such that for all $\lambda\in \Re$
\begin{align*}
E[e^{\lambda X}]\leq e^{\frac{\lambda^2\sigma^2}{2}+\lambda\mu}
\end{align*}
Prove that $E[X]=\mu$.
\iffalse
\begin{enumerate}
	\item $E[X]=\mu$.
	\item $\var(X)\leq \sigma^2$.
	\item If the smallest value of $\sigma$ satisfying the above equation is chosen, is it true that $\var(X)=\sigma^2$? Prove or give a counter example.
\end{enumerate}
\fi
%	\newpage
	\ib
	
	
	\newpage
	\section{Long questions (33 points)}
\textbf{Please answer any three of the long questions.}
	\bi
	\item (11 pts) Look at the seminar paper ``Probability Inequalities for Sums of Bounded Random Variables'' by Wassily Hoeffding. It should be available via \url{lib.utexas.edu}. You can assume that $n$ is a multiple of $m$ (the degree of the kernel). Assume that the kernel is bounded, i.e. $|h(X_1,\dots,X_m)-\theta|\leq b$, where $\theta=E[h(X_1,\dots, X_m)]$.
	\begin{enumerate}
		\item (4 pts) Read and reproduce the proof of equation 5.7 for large sample deviation of order $m$ U statistics. 
		\item (7 pts) Also prove Bernstein's inequality (see below) for U statistics. This is buried in the paper, you will have to find the bits and pieces and put them together. The Bernstein inequality is given by:
		\begin{align*}
		P(|U_n-\theta|\geq \epsilon)\leq a\exp\left(-\frac{ n \epsilon^2/m}{c_1\sigma^2+c_2 \epsilon}\right),
		\end{align*}
		where $\sigma^2=\var(h(X_1,\dots,X_m))$ and $a,c_1,c_2$ are universal constants.
	\end{enumerate}
	\item (11 pts)
	Consider a random undirected network, where $A_{ij}=A_{ji}\stackrel{iid}{\sim} Bernoulli(p_n)$ for $1\leq i<j\leq n$. $A_{ii}=0$ for $1\leq i\leq n$. The degree of a node is defined as $d_i=\sum_j A_{ij}$. Consider the regime where $np_n/\log n\rightarrow \infty$. \textit{Hint: remember, not all concentration inequalities work in this regime.}
	
	
	\bi
	\item (5 pts) Show that the degree of a fixed node concentrates around its expectation $(n-1)p_n$. Obtain the tail bound explicitly.
%	Your bound should be of the form:
%	$$P(|d_i-(n-1)p_n|\geq \sqrt{cnp_n\log n})\leq n^{-C},$$
%	where $c$ and $C$ are positive constants.
%	Use Bernstein's inequality to get
%	$$P(|d_n-(n-1)p_n|\geq t)\leq 2\exp(-\frac{t^2/2}{np_n+t/3})$$
	
%	Using $t=\sqrt{4cnp_n\log n}$, the above tail becomes $n^{-c}$.
	\item (4 pts) Can you obtain a uniform error bound on the degrees? That is, can you show that $\max_i \dfrac{|d_i-(n-1)p_n|}{(n-1)p_n}$ goes to zero in probability? If not, show why not. If yes, obtain the tail bound. 
	
	%Using the union bound,
%	$P(\max_i |d_i-(n-1)p_n|\geq \sqrt{cnp_n\log n})\leq n^{1-C}$.
%	What value of $c$ will you use to have concentration? %Using $c=2$, the above tail becomes $n^{-1}$.
%	\vspace{5in}
	\item (2 pts) Denote by $d_{(n)}$ the maximum degree. Using the last two questions, show that the maximum degree also concentrates. Obtain the tail bound explicitly. 
	
%	With probability at least $1-1/n$,
	
%	$|d_{(n)}-(n-1)p_n|\leq \max_i |d_i-(n-1)p_n|\leq \sqrt{8np_n\log n}$.
	\ib
\item 	(11 pts)
We will go back to finding the covering number of infinite dimensional ellipses in this problem.  Given a collection of positive numbers $\{\mu_j,j=1\dots d\}$, consider the ellipse $$\mathcal{E}_d=\{\theta\in\R^d : \sum_i \theta_i^2/\mu_i^2\leq 1\},$$ specified by the sequence $\mu_j = j^{-2\beta}$
for some parameter $\beta > 1/2$.
\bi
\p (5 pts) Obtain an upper bound on the $\epsilon$ packing number of $\mathcal{E}_d$ under an appropriate distance metric. What metric do you think you should use? 
\p (6 pts) Now consider an infinite-dimensional ellipse $\mathcal{E}$, specified by the sequence $\mu_j = j^{-2\beta}$
for some parameter $\beta > 1/2$. Show that
$$\log N(\epsilon; \mathcal{E},\|.\|_{\ell_2})\leq C \left(\frac{1}{\epsilon}\right)^{1/\beta},$$
where $\|\theta-\theta'\|_{\ell_2}^2=\sum_{i=1}^\infty (\theta_i-\theta_i')^2$ is the squared $\ell_2$-norm on the space of square summable sequences.
\ib

\item (11 pts) Consider a random undirected network, where $A_{ij}=A_{ji}\stackrel{iid}{\sim} Bernoulli(p)$ for $1\leq i<j\leq n$. $A_{ii}=0$ for $1\leq i\leq n$. Let $T$ denote the number of triangles in this graph.
\bi
\p (6 pts)  Show that the variance of $T$ is $${n\choose 3}(p^3-p^6)+c_1{n\choose 4}(p^5-p^6),$$
where $c_1$ is a universal constant.
\p (5 pts) Now use the Efron Stein inequality to obtain an upper bound on the variance. Use the true variance as a guideline to get a tight upper bound.

\ib
%\newpage\phantom{blabla}
\end{enumerate}	
	
%	\newpage
	%\begin{center}
	
	
\end{document}
