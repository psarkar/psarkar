<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="CoffeeCup HTML Editor (www.coffeecup.com)">
    <meta name="dcterms.created" content="Wed, 21 Jan 2015 05:15:16 GMT">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <title></title>
    
    <!--[if IE]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <title>Other</title>


  <link rel="stylesheet" type="text/css" href="style.css" media="screen" />

</head>



<body>

<div id="header">
<h1>Purnamrita Sarkar</h1>

<div id="menu">
<ul id="nav">

  <li><a href="index.html">Home</a></li>

  <li></li>

  <li><a href="Research1.html">Research</a></li>

  <li><a href="teaching.html">Teaching<span style="font-weight: bold;"></span></a></li>

</ul>

</div>

</div>


<div id="content">
<div style="color: rgb(0, 0, 0);" id="right">
<h2><a name="3"></a>SDS 383C: Statistical Modeling I</h2>
<br />
The course syllabus can be found <a href="sds383c_16/syllabus.pdf"><span style="font-weight: bold;">here. </span> </a><br />
<br/>
The scribing material can be found in <a href="sds383c_16/scribe.zip"><span style="font-weight: bold;">here. </span></a>
<br/><br/>
<span style="font-weight: bold;">08/25 </span>: Different types of models and convergence of random variables <a href="sds383c_16/lecture1.pdf"> [Scribe notes]. </a> 6.2 and 5.1-5.2 from All of Statistics<br />
<br/>
<span style="font-weight: bold;">08/30 </span>: Properties of the MLE.  5.3, 9.3-9.10 from All of Statistics, <a href="sds383c_16/lecture2_scribe.pdf"> [Scribe notes]. </a> <br />
<br/>

<span style="font-weight: bold;">09/1 </span>: Multivariate random variables, Parametric Bootstrap from All of Statistics, <a href="sds383c_16/lecture3_scribe.pdf"> [Scribe notes]. </a>
<br/><br/>
<span style="font-weight: bold;">09/3 </span>: First homework is out. <a href="sds383c_16/HW1.pdf"> [HW1]. </a> Dataset <a href="sds383c_16/dir1.txt"> [dir1.txt]. </a>  </a> <a href="sds383c_16/HW1-sol.pdf"> [Solutions, thanks to Giorgio Paulon!]. </a>  <br />



<br/>
<span style="font-weight: bold;">09/13 </span>: Failings of the MLE. Shrinkage estimators and empirical Bayes.  5.3, 9.3-9.10 from All of Statistics, <a href="sds383c_16/lecture4_scribe.pdf"> [Scribe notes]. </a><br />
<br/>
<span style="font-weight: bold;">09/15 </span>: Empirical Bayes and James Stein estimators more closely, Linear regression, Hastie-Tibshirani-Friedman 3.1-3.2.2<a href="sds383c_16/lecture5_scribe.pdf"> [Scribe notes]. </a><br />
<br/>
<span style="font-weight: bold;">09/20 </span>: Linear regression, Hastie-Tibshirani-Friedman 3.1-3.2.2<a href="sds383c_16/lecture6_scribe.pdf"> [Scribe notes]. </a><br />
<br/>
<span style="font-weight: bold;">09/25 </span>: Second homework is out. <a href="sds383c_16/HW2.pdf"> [HW2]. </a> <a href="sds383c_16/HW2-sol.pdf"> [Solutions, thanks to Spencer Woody!]. </a>  <br />
<br/>

<span style="font-weight: bold;">09/22 </span>: Model selection. Optimism of training error. H-T-F 7.1 , 7.4 7.5 <a href="sds383c_16/lecture7_scribe.pdf"> [scribe notes]. </a>
<br/>
<br/>
<span style="font-weight: bold;">09/23 </span>: Model selection continued. AIC, BIC, Variable selection, H-T-F 7.10 , 3.4 <a href="sds383c_16/lecture8_scribe.pdf"> [scribe notes]. </a>
<br/>
<br/>
<span style="font-weight: bold;">09/23 </span>:  Cross Validation. Ridge regression and Lasso H-T-F 7.10 , 3.4  <a href="sds383c_16/lecture9_scribe.pdf"> [scribe notes]. </a>
<br/>
<br/>

<span style="font-weight: bold;">09/27 </span>:  Ridge regression and the Lasso - orthogonal coefficients. H-T-F 7.10 , 3.4 <a href="sds383c_16/lecture10_scribe.pdf"> [scribe notes]. </a>
<br/>


<br/>
<span style="font-weight: bold;">09/29 </span>: Logistic regression. H-T-F 4.4 <a href="sds383c_16/lecture11_scribe.pdf"> [scribe notes]. </a>
<br/><br/>

<span style="font-weight: bold;">10/3 </span>: Third homework is out. <a href="sds383c_16/HW3.pdf"> [HW3]. </a> Dataset <a href="sds383c_16/hw3_data2.zip"> [Breast cancer data].  </a> </a> <a href="sds383c_16/HW3-sol.pdf">Solutions. </a>
<br/>

<br/>
<span style="font-weight: bold;">10/4 </span>: Collinearity in lasso/ridge and introduction to LDA <a href="sds383c_16/lecture_scribe12.pdf"> [scribe notes]. </a>
<br/><br/>
<span style="font-weight: bold;">10/11 </span>: LDA and Fisher discriminant analysis HTF 4.3 <a href="sds383c_16/lecture_scribe13.pdf"> [scribe notes]. </a>
<br/>
<br/>
<span style="font-weight: bold;">10/13 </span>: Old Midterm  <a href="sds383c_16/midterm-shorter-soln.pdf"> [Solutions]. </a> 
<br/>
<br/>
<span style="font-weight: bold;">10/13 </span>: Some sample midterm questions  <a href="sds383c_16/sample.pdf"> [sample questions]. </a>
 <br/><br/>

<span style="font-weight: bold;">10/18 </span>: Naive Bayes. Generalized linear models. Fitting.  <a href="sds383c_16/lecture_scribe14.pdf"> [scribe notes]. </a>

 Tom Mitchell's book chapter on Naive Bayes can be found <a href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf"> [here]. </a> <br/><br/>

<span style="font-weight: bold;">10/20 </span>: Robust Statistics - sensitivity curve, influence function and breakdown point. Huber loss. M estimation.  <a href="sds383c_16/lecture_scribe15.pdf"> [scribe notes]. </a>
 <br/><br/>
<span style="font-weight: bold;">10/22 </span>: M estimation continued. Models for clustering--kmeans   <a href="sds383c_16/lecture15_scribe.pdf"> [scribe notes]. </a>
 <br/>
<br/>
<span style="font-weight: bold;">10/25 </span>: Midterm  <a href="sds383c_16/midterm-2016-sol.pdf"> [Solutions]. </a> 
<br/>

<br/>
<span style="font-weight: bold;">10/27 </span>: EM, missing data and censoring  <a href="sds383c_16/lecture16_em_scribe.pdf"> [scribe notes].</a>
<br/>

<br/>
<span style="font-weight: bold;">10/27 </span>: Fourth homework is out. <a href="sds383c_16/HW4.pdf"> [HW4]. </a>
<br />

<br/>
<span style="font-weight: bold;">10/28 </span>: Project guidelines. The project reports are due on the last day of class. <a href="sds383c_16/project.pdf"> [project guidelines]. </a>
<br />
<br />
<span style="font-weight: bold;">11/08 </span>: When EM may not work, lightbulb example  <a href="sds383c_16/lecture18.pdf"> [scribe notes]. </a> Flury and Zoppe's  <a href="http://www.webpages.uidaho.edu/~stevel/565/literature/Exercise%20in%20EM.pdf">word of caution</a> on EM.
<br/>
<br/>

<span style="font-weight: bold;">11/10 </span>: d-separation demystified <a href="http://www.autonlab.org/_media/tutorials/bayesinf05.pdf">slides</a> .

<br/>
<br/>
<span style="font-weight: bold;">11/15 </span>: Gibbs and collapsed gibbs for a Naive Bayes type document model. Gibbs for the Latent Dirichlet Allocation model.    <a href="sds383c_16/lecture19.pdf"> [scribe notes]. </a> A great primer to Gibbs sampling for the uninitiated <a href="https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf">here</a>. Check for collapsed gibbs sampling under 2.4.3 and 2.5.1. Check out the trouble in integrating out other continuous parameters under 2.6.
<br/>
<br/>

<span style="font-weight: bold;">11/17 </span>: Gibbs and collapsed gibbs for a the Latent Dirichlet Allocation model.    <a href="sds383c_16/lecture22.pdf"> [scribe notes]. </a> A great source is  <a href=" https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf">here</a>. 
<br/>
<br/>
<span style="font-weight: bold;">11/17 </span>: More project guidelines: projects should be written in the NIPS format which can be downloaded   <a href="https://nips.cc/Conferences/2016/PaperInformation/StyleFiles"> [Here]. </a> The length can be at most 8 pages plus 1 page of references. In the introduction, tell us what problem you are intending to solve and why its important. In the related work section talk in detail about the other works that are relevant. In Proposed work, tell me concretely what it is you are trying to solve, or which methods are you surveying. In experiments, compare the different methods on simulated and real datasets. And finally in Discussion, tell me what you have learned, what works better and when or why. The deadline is 5th Dec midnight, but I will not start grading until 9th Dec, morning.  
<br/>

<br/>
<span style="font-weight: bold;">11/15 </span>: Fifth homework is out -- it is due Dec 5th. <a href="sds383c_16/HW5.pdf"> [HW5]. </a>  <br />
<br/>
<span style="font-weight: bold;">11/22 </span>: Bootstrap and subsampling. <a href="sds383c_16/lecture23.pdf"> [scribe notes].</a> A great source for bootstrap is <a href="https://normaldeviate.wordpress.com/2013/01/19/bootstrapping-and-subsampling-part-i/"> here. </a> The second part of the blog post talks about subsampling. Also chapter 8 of A-S is a good resource.

</div>

</body>
</html>

  </body>
</html>
